---
layout: post
title: 自动驾驶小讲座
date: 2018-10-25
author: 阿金
tags: 多机智能
---

# 公司网络分割分享

师兄回来给大家讲一讲他们做的工作。

## 一些想法

认为自动驾驶主要以服务的形式提供。以后大家不需要有私家车，更多是需要从租赁公司租赁。

## 现有布局

1. 产业界2020实现L3L4，产业机会在2020-2022之间。

2. 以低打高的方式不太可行，ADAS的思路和自动驾驶的思路还是完全不同。还是应该以高打低，先做L4,L5然后降级到L2L3，上主机厂进行合作。

3. “双百俱乐部”，一百辆车，一百万mile路测。只有四家：google、mobileeye。

## 自主驾驶和自动驾驶

现在都是基于高精度地图和有限规则驾驶手册的自动驾驶，但是离自主驾驶还比较远。

自主驾驶：需要在线构图和离线构图的耦合关系。需要高智能处理长尾和模糊边界问题。

**在线认知与智能决策能力**是自动向自主的关键。

### 在线认知

静态场景推理：与离线形成互补，对未知道路来适应。比如雪地中，没法检测车道线，只能靠车辙。或者下了大雨，路面积水。

动态场景理解：对变换剧烈的场景进行预测。

驾驶场景预测：非结构化道路，有人不遵守交通规则的情况下。

### 规划决策控制

几层划分，逻辑清楚。

![决策规划](/post_img/20181025/desi.jpg)

当前有两个问题：

1. 保守的驾驶策略==安全？保守策略只能主动安全，不能被动安全。因为人的开车存在博弈，比如并线，人开车会尝试抢一抢位置，能够拉扯出空间之后才会并线。自动驾驶车，在早晚高峰，永远无法左转。

2. 不完美感知如何决策规划：传感器鲁棒性。

## 强化学习如何助力真正的自动驾驶

交规就是专家经验（规则集）是死限。

中间一些模糊边界的问题，或者在交规限制里面，利用强化学习进行探索。

技术方案：
分为4个步骤。

1. 示教学习，具备初级驾驶能力。
2. 仿真环境增强学习，在仿真环境中提高能力（当前没有一个仿真环境，能够适应自动驾驶需求）
3. 实际工况的驾驶策略
4. 小样本、个性化驾驶策略

## 自动驾驶工作

### 端到端自动驾驶解决方案

输入图片，输出方向盘转向。但是这个方案基本凉了。【如果是纯图像的，训出来的结果与实际场景偏差不大。】不同的车模型也不一样。

### 非结构化的道路检测

不再通过rawdata来控制小车，需要提取环境信息来进行训练。训练刹车、油门、方向盘的增量。

### 强化学习自动泊车工况范围

现在的泊车规划：三段式圆弧泊车。主要挑战是要找第一阶段。

用的是 PPO 算法。

自动泊车的性能要比人工好。

### 模糊边界：变道

### 多车博弈：无左转灯的左转

## 未来工作

### 强化学习缺点

1. 不可复现：
    * 泛化能力太差。
        * 多任务
        * 课程学习（在不同task情况下限定domain）
        * 元学习（meta-learning）

2. 训练难：
    * off-policy方式数据效率低。
        * 数据回放管理
        * 更高效率的异步采样
        * value-based与policy-based相结合
    * N函数逼近本身存在问题。
        * God Bless

3. 不可解释：
    * model-free的数据利用效率和可解释性差。
        * model-base 与 model-free 相结合
        * 提升模型探索效率
        * Policy-reuse

### 研究课题

#### 数据高效模仿学习

reward设计困难，通过算法来掌握示教数据。

#### 多车博弈
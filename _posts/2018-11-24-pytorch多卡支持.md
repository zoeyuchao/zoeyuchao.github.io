---
layout: post
title: pytorch多卡支持
date: 2018-11-24
author: 阿金
tags: 深度学习
---

# pytorch多卡支持

## 声明整个网络 和 网络参数加载：

```Python
def gen_ResNetVlad():
    use_cuda = True
    
    model = ResNetVlad() #定义一个网络
    if use_cuda: 
        model = model.cuda() #选择cuda运行 
        if torch.cuda.device_count() > 1: #添加多卡支持
            print('Multi_gpu')
            model = torch.nn.DataParallel(model)
    
    # state_dict = torch.load('vd16_data_wpca.pkl') # add map_location='cpu' if no gpu

    #state_dict = torch.load('./models/student_net_params_res18_init.pkl')
    #读取网络参数
    state_dict = torch.load('./output_log1132/student_net_multi_params_epoch_5.pkl')
    print(state_dict)

    #加载网络参数
    model.load_state_dict(state_dict)
    return model
```

## 保存网络代码如下

```Python
# 打印网络参数
for name,p in student.named_parameters():
    if name == 'module.WPCA.bias':
        print( name, p)

# 保存网络
torch.save(student.state_dict(), './output_log/student_net_params_epoch_{epoch}.pkl'.format(epoch=epoch))
```

## 遇到问题

用单个cpu的时候保存的网络，在使用多个cpu运行的时候，会报错。同样的，使用多个CPU保存的网络，也无法在单个CPU运行的时候读取。

因为：

```Python
model = torch.nn.DataParallel(model)
```

这句话会把原始的model再分装一层。导致 单GPU和多GPU训练的网络并不是同一网络，里面的参数名也会有不同。

比如：

单GPU的参数名称为 conv1.bias，多GPU的参数名称 为 module.conv1.bias。

## 解决方案

调节多GPU网络定义的数字

```Python
model = torch.nn.DataParallel(model)
```

和

```Python
    #读取网络参数
    state_dict = torch.load('./output_log1132/student_net_multi_params_epoch_5.pkl')
    print(state_dict)

    #加载网络参数
    model.load_state_dict(state_dict)
    return model
```

的位置，可以选择读取的网络是单GPU和多GPU。